{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0b7bd0",
   "metadata": {},
   "source": [
    "Implement and train Softmax Regression with mini-batch SGD and early stopping.\n",
    "\n",
    "The expected outcome.\n",
    "* Implement Softmax Regression Model.\n",
    "* Implement mini-batch SGD.\n",
    "* The training should support early stopping.\n",
    "* Train and evaluate the model with cross-validation. The evaluation metric is the *accuracy*.\n",
    "* Retrain the model with early stopping.\n",
    "\n",
    "\n",
    "**DO NOT USE SKLEARN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c066137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f4f1598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "df = pd.DataFrame({fname: values for fname, values in zip(iris[\"feature_names\"], X.T)})\n",
    "df[\"target\"] = y\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c595d",
   "metadata": {},
   "source": [
    "## Your Code\n",
    "You can start writing your code from here. Please don't modify any of the previous code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4072a3b",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploraring Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9fd54c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   target             150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1464d8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "features = df.columns.tolist()\n",
    "features.remove('target')\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dcd7a336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Unique\n",
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf4dda",
   "metadata": {},
   "source": [
    "So, it's a multi-class Classification, so one-hot encoding is required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb817f89",
   "metadata": {},
   "source": [
    "---\n",
    "# Splitting Data:\n",
    "\n",
    "Split Data randomly to training set, validiation set and Testing set.\n",
    "\n",
    "### Required Libararies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ba8c32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to fix random seed:\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4af28bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, test_size = 0.2):\n",
    "    \"\"\" \n",
    "    Split dataset into train-test sets\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset : list\n",
    "              Vector size (m, 1)\n",
    "    \n",
    "    test_size : float\n",
    "                Specify the ratio of test set.\n",
    "                \n",
    "    Returns:\n",
    "    -------\n",
    "    train : list\n",
    "            The training set.\n",
    "            \n",
    "    dataset_copy : list.\n",
    "                   The test set.\n",
    "                    \n",
    "    \"\"\"\n",
    "    \n",
    "    train = list()\n",
    "    train_size = (1-test_size) * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = random.randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8ccd67de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X_train =  (96, 4) \tX_valid =  (24, 4) \tX_test =  (30, 4)\n",
      "shape y_train =  (96, 1) \ty_valid =  (24, 1) \ty_test =  (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Firstly Split all dataset into Training and testing set.\n",
    "X_ , X_test = train_test_split(X, test_size = 0.2)\n",
    "y_ , y_test = train_test_split(y, test_size = 0.2)\n",
    "\n",
    "# Secondly split the training set and validation set.\n",
    "X_train , X_valid = train_test_split(X_, test_size = 0.2)\n",
    "y_train , y_valid = train_test_split(y_, test_size = 0.2)\n",
    "\n",
    "# Output sets are of type List\n",
    "\n",
    "# Printing\n",
    "print(\"shape X_train = \", pd.DataFrame(X_train).shape,\"\\tX_valid = \", pd.DataFrame(X_valid).shape,\"\\tX_test = \", pd.DataFrame(X_test).shape)\n",
    "print(\"shape y_train = \", pd.DataFrame(y_train).shape,\"\\ty_valid = \", pd.DataFrame(y_valid).shape,\"\\ty_test = \", pd.DataFrame(y_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d17ec4",
   "metadata": {},
   "source": [
    "---\n",
    "# One-Hot Encoding Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0525cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(col):\n",
    "    \"\"\" \n",
    "    Encode column into encoded matrix corresponds to the input col.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    col : List\n",
    "          Vector size (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    softmax_arr : list\n",
    "                  The output will be a sparse matrix where each column corresponds to one possible value of one feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    col_len =len(col)\n",
    "    n = max(col) + 1 # +1 : At least one column\n",
    "    \n",
    "    # Create Zeros Matrix of rows = size, columns = n\n",
    "    oneHot_x = np.zeros((col_len, n)) \n",
    "    \n",
    "    # Assigning 1 for each Category and let others to 0.\n",
    "    oneHot_x[np.arange(col_len), col] = 1\n",
    "    \n",
    "    return oneHot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "37251932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing:\n",
    "oneHotEncoding(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9228a89",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Softmax Regression Model.\n",
    "\n",
    "$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "809f0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" \n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : List\n",
    "        the dataset size (m, n).\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    softmax_arr : List\n",
    "                  The softmax values for the input array list.\n",
    "                  \n",
    "    Note:\n",
    "    ----\n",
    "    Subtracting max(x) from x leaves a vector that has only non-positive entries, \n",
    "    ruling out overflow and at least one element that is zero ruling out a vanishing denominator.\n",
    "    \"\"\"\n",
    "    # subtracting the max of x for numerical stability.\n",
    "    exp = np.exp(x - np.max(x))\n",
    "\n",
    "    # Calculating softmax for all examples.\n",
    "    for i in range(len(x)):\n",
    "        exp[i] /= np.sum(exp[i])\n",
    "        \n",
    "    return exp    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea4165",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross-Validation Implementation\n",
    "\n",
    "Split a dataset into k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ce0d787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, folds=3):\n",
    "    \"\"\" \n",
    "    Split dataset into K-folds.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset : List\n",
    "              The dataset size (m, n).\n",
    "    \n",
    "    folds : Integer\n",
    "            The number of folds to split the dataset.\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    dataset_split : List\n",
    "                    The splitted datasets.\n",
    "                  \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "372c6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3], [5]], [[4], [10]], [[2], [8]], [[1], [6]]]\n"
     ]
    }
   ],
   "source": [
    "# test cross validation split\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "folds = cross_validation_split(dataset, 4)\n",
    "print(folds)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5a72a",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement mini-batch SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8a5c91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDMiniBatch(x, y, epochs, batch_size = 4, lr = 0.1, percision = 0.1):\n",
    "    \n",
    "    \"\"\" \n",
    "    Stocastic mini batch Gradient decent implementation to get the best coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    x : List\n",
    "        Feature list, size (m, n).\n",
    "    \n",
    "    y : List\n",
    "        Target list.\n",
    "    \n",
    "    epochs : Integer\n",
    "             Number of maximum iteration.\n",
    "    \n",
    "    batch_size : Integer\n",
    "                 Number of batches to split into.\n",
    "                 \n",
    "    lr : Float\n",
    "         Learning Rate factor. Default value = 0.1\n",
    "         \n",
    "    percision : Float\n",
    "                Percision to stop the gradient decent\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    theta : List\n",
    "            Coefficient List.\n",
    "                  \n",
    "    \"\"\"\n",
    "    \n",
    "    # First define theta Matrix, rows : coef of each feature of x. cols : coef for each output y encoded.\n",
    "    theta_rows = np.array(X_train).shape[1]\n",
    "    #theta_cols = np.array(y_train).shape[1]\n",
    "    theta = np.zeros((theta_rows + 1, 1),dtype = np.float64) \n",
    "    \n",
    "    y_batch = np.array_split(y, batch_size)\n",
    "    \n",
    "    # Adding First col of ones(Bias)\n",
    "    X = np.column_stack((np.ones(len(x)),x))\n",
    "    X_batchs = np.array_split(X,batch_size)\n",
    "    \n",
    "    best_loss = np.infty\n",
    "    \n",
    "    for i in range(epochs): \n",
    "        # Looping over each barch.\n",
    "        for miniIndex in range(batch_size):\n",
    "            # Selecting mini Batch\n",
    "            mini_x, mini_y = X_batchs[miniIndex], y_batch[miniIndex]\n",
    "    \n",
    "            # Encode the target batch\n",
    "            y_oneHot = oneHotEncoding(mini_y)\n",
    "            \n",
    "            # Calculate hypothes.\n",
    "            hypoth = mini_x.dot(theta)\n",
    "            \n",
    "            # get Softmax of hypoth\n",
    "            y_proba = softmax(hypoth)\n",
    " \n",
    "            cost = -np.mean(np.log1p(y_proba))\n",
    "            \n",
    "            error = y_proba - y_oneHot\n",
    "            \n",
    "            gradient = (1/(len(mini_x))) * (mini_x.T.dot(error))\n",
    "            \n",
    "            theta = theta - lr*gradient\n",
    "            \n",
    "            if cost < best_loss:\n",
    "                best_loss = cost\n",
    "                if cost < percision: \n",
    "                    print(\"Iteration #\",i, cost, \"early stopping!\")\n",
    "                    break\n",
    "             \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4024f52b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0 -0.6931471805599453 early stopping!\n"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "theta = SGDMiniBatch(X_train,y_train,500,5,0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "abe44c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.55992187, -0.85002504, -1.90989683],\n",
       "       [-1.56298807,  0.23318396,  0.20280411],\n",
       "       [ 0.63951006, -1.0842149 , -0.20529517],\n",
       "       [ 0.49787864, -0.65169692, -0.43918172],\n",
       "       [ 0.49673609, -0.35734685, -0.30638925]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47104f3",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0a8b165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    \"\"\" \n",
    "    predict target for samples\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    x : List\n",
    "        Feature list, size (m, n).\n",
    "    \n",
    "    theta : List\n",
    "            Coeficients list.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    y_predict : List\n",
    "                Predicted Target List.\n",
    "                  \n",
    "    \"\"\"\n",
    "     # Adding First col of ones(Bias)\n",
    "    X = np.column_stack((np.ones(len(x)),x))\n",
    "    \n",
    "    s = X.dot(theta)\n",
    "    \n",
    "    y_proba = softmax(s)\n",
    "    y_predict = np.argmax(y_proba, axis=1)\n",
    "    \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0804a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 2], dtype=int64)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Predict:\n",
    "y_predict = predict(X_valid,theta)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4a3ee",
   "metadata": {},
   "source": [
    "# Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0994cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_accuracy(y_predict, y_true):\n",
    "    \"\"\" \n",
    "    compute Accuracy between predicted and true values.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_predict : List\n",
    "                Perdicted list, size (m, n).\n",
    "    \n",
    "    y_true : List\n",
    "             Target list.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    accuracy_score : float\n",
    "                     Accuracy Value.\n",
    "                  \n",
    "    \"\"\"\n",
    "    accuracy_score = np.mean(y_predict == y_true)\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "05e927fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Score:\n",
    "score = score_accuracy(y_predict, y_valid)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "53b571a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = predict(X_test,theta)\n",
    "score = score_accuracy(y_test_predict, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f51eb831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb4956",
   "metadata": {},
   "source": [
    "Using the following cell to train and evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "626be344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0 -0.693147180559945 early stopping!\n",
      "[0 2 1 1 0 1 0 0 2 2 2 2 2 1 0 0 0 1 1 2 0 2 1 2 2 1 1 0 2 0]\n",
      "Accuracy = 0.9666666666666667\n",
      "--------------------\n",
      "Iteration # 0 -0.693147180559945 early stopping!\n",
      "[2 2 1 2 0 2 0 1 0 2 0 0 2 1 2 1 0 0 1 2 0 2 1 0 1 2 2 0 2 2]\n",
      "Accuracy = 0.9\n",
      "--------------------\n",
      "Iteration # 0 -0.693147180559945 early stopping!\n",
      "[2 0 2 2 1 0 1 0 0 2 0 1 2 0 0 2 2 2 1 2 1 0 0 1 2 1 2 1 0 2]\n",
      "Accuracy = 0.9333333333333333\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df[\"target\"]):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "    \n",
    "    # Use strat_train_set and strat_test_set to train and evaluate your model\n",
    "    xtrain=strat_train_set.iloc[:,:4].values\n",
    "    ytrain=strat_train_set.iloc[:,4].values\n",
    "\n",
    "    xtest=strat_test_set.iloc[:,:4].values\n",
    "    ytest=strat_test_set.iloc[:,4].values\n",
    "    \n",
    "    # Fit to model : choose parameters\n",
    "    epochs, batch_size, learning_rate, percision = 1000, 4, 0.01, 0.001\n",
    "    theta = SGDMiniBatch(xtrain, ytrain, epochs, batch_size, learning_rate, percision)\n",
    "    \n",
    "    # Prediction\n",
    "    y_predict = predict(xtest,theta)\n",
    "    \n",
    "    # Accuracy Score\n",
    "    score = score_accuracy(y_predict, ytest)\n",
    "    \n",
    "    print(y_predict)\n",
    "    print(\"Accuracy =\", score)\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dcb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b14b0ea7",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing SoftMax Regression Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8b8f3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax training accuracy: 0.5208333333333334\n",
      "Softmax validation accuracy: 0.4583333333333333\n",
      "Softmax test accuracy    : 0.13333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit softmax classifier\n",
    "\"\"\"\n",
    "    if multi_class is set to be “multinomial”,\n",
    "    the softmax function is used to find the predicted probability of each class\n",
    "\"\"\"\n",
    "lr_mn = LogisticRegression(multi_class = \"multinomial\")\n",
    "lr_mn.fit(pd.DataFrame(X_train), pd.DataFrame(y_train))\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax validation accuracy:\", lr_mn.score(X_valid, y_valid))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "24735cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax training accuracy: 0.975\n",
      "Softmax test accuracy    : 0.9666666666666667\n",
      "Softmax training accuracy: 0.975\n",
      "Softmax test accuracy    : 0.9666666666666667\n",
      "Softmax training accuracy: 0.9833333333333333\n",
      "Softmax test accuracy    : 0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Yousri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Yousri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "ahmed = 1\n",
    "for train_index, test_index in split.split(df, df[\"target\"]):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "    \n",
    "    # Use strat_train_set and strat_test_set to train and evaluate your model\n",
    "    xtrain=strat_train_set.iloc[:,:4].values\n",
    "    ytrain=strat_train_set.iloc[:,4].values\n",
    "\n",
    "    xtest=strat_test_set.iloc[:,:4].values\n",
    "    ytest=strat_test_set.iloc[:,4].values\n",
    "    \n",
    "    lr_mn = LogisticRegression(multi_class = \"multinomial\")\n",
    "    lr_mn.fit(pd.DataFrame(xtrain), pd.DataFrame(ytrain))\n",
    "\n",
    "    print(\"Softmax training accuracy:\", lr_mn.score(xtrain, ytrain))\n",
    "    #print(\"Softmax validation accuracy:\", lr_mn.score(X_valid, y_valid))\n",
    "    print(\"Softmax test accuracy    :\", lr_mn.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0dea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
